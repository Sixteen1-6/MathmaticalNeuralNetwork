Neural Networks: From Black Box to Glass Box

Transforming the mysterious "black box" of neural networks into a transparent glass box through comprehensive mathematical derivations



Overview: This repository contains a comprehensive exploration of neural networks. It starts from the basics of linear regression and builds all the way up to backpropagation with every step derived.


What You Can Learn: Linear Regression: Matrix formulations and the normal equation
Logistic Regression: Sigmoid functions and cross-entropy loss
Neural Networks: Forward propagation and backpropagation algorithms
The Universal Pattern: How all methods follow the same core principle: Model -> Loss -> Gradient -> Update

You can view an interactive presentation by opening presentation/index.html 


Otherwise, you can explore the complete mathematical derivations found in the docs/ file.

Also there is a section on the complete transformation of the mathmatical derivations to python code directly. This would be beneficial for a complete beginner to understand the foundations.


MathematicalNeuralNetwork/
â”œâ”€â”€ ðŸ“„ README.md                          # You are here!
â”œâ”€â”€  docs/                              # Mathematical derivations
â”‚   â”œâ”€â”€  linear-regression-derivation.pdf
â”‚   â”œâ”€â”€  logistic-regression-derivation.pdf
â”‚   â””â”€â”€  neural-network-backpropagation.pdf
â”œâ”€â”€  presentation/                       # Interactive experience
â”‚   â”œâ”€â”€  index.html                     # Main                     
â”œâ”€â”€  notebooks/                         # Implementation
â”‚   â”œâ”€â”€  NeuralNetworkCode-Walkthrough.           # Complete implementation

