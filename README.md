Neural Networks: From Black Box to Glass Box

Transforming the mysterious "black box" of neural networks into a transparent glass box through comprehensive mathematical derivations



Overview: This repository contains a comprehensive exploration of neural networks. It starts from the basics of linear regression and builds all the way up to backpropagation with every step derived.


What You Can Learn: Linear Regression: Matrix formulations and the normal equation
Logistic Regression: Sigmoid functions and cross-entropy loss
Neural Networks: Forward propagation and backpropagation algorithms
The Universal Pattern: How all methods follow the same core principle: Model -> Loss -> Gradient -> Update

You can view an interactive presentation by opening presentation/index.html 


Otherwise, you can explore the complete mathematical derivations found in the docs/ file.

Also, there is a section on the complete transformation of the mathematical derivations to Python code directly. This would be beneficial for a complete beginner to understand the foundations.

Project Directory Structure:
```
MathematicalNeuralNetwork/
├── README.md                          # You are here!
├── docs/                              # Mathematical derivations
│   ├── linear-regression-derivation.pdf
│   ├── logistic-regression-derivation.pdf
│   └── neural-network-backpropagation.pdf
├── presentation/                      # Interactive experience
│   └── index.html                     # Main
├── notebooks/                         # Implementation
│   └── NeuralNetworkCode-Walkthrough. # Complete implementation
```


